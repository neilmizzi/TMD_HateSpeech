{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "NB: Before starting, make sure to install all necessary requirements and Twint on your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Skip block if all requirements are already satisfied\n",
    "import sys\n",
    "!pip3 install -r twint/requirements.txt\n",
    "!pip3 install twint\n",
    "!pip3 install nest-asyncio\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/neilmizzi/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/neilmizzi/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install NLTK library and run the following to download NLTK-required content\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports\n",
    "### Initialising variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Scraping \n",
    "### Config and Execution\n",
    "\n",
    "Info about the configuration options for both Twint, \n",
    "the Tweets to look up and the User options can be found at\n",
    "https://github.com/twintproject/twint/wiki/Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Search Done. Output in ./data.csv\n"
    }
   ],
   "source": [
    "file_dir = './data.csv'\n",
    "c = twint.Config()                  \n",
    "c.Store_csv = True                      # Store as CSV\n",
    "c.Output = (file_dir)                   # Output file directory\n",
    "c.Limit = 100                           # No. of Tweet Limit\n",
    "c.Hide_output = True                    # Hide Output\n",
    "c.Pandas = True\n",
    "c.Username = 'realdonaldtrump'          # Username to look up\n",
    "\n",
    "\n",
    "twint.run.Search(c)\n",
    "tweets = twint.storage.panda.Tweets_df\n",
    "print(\"Search Done. Output in \"+ file_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "NB: All the fields from the Twint API can be seen here:\n",
    "\n",
    "https://github.com/twintproject/twint/wiki/Tweet-attributes#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "695    Mini Mike Bloomberg’s debate performance tonig...\n696    Thank you Arizona! With your help, your devoti...\n697    “What has happened to Roger Stone should never...\n698                           pic.twitter.com/1XnhyX8jb2\n699    The Democrat party is the party of high taxes,...\nName: tweet, dtype: object"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Fields as necessary\n",
    "data = pd.read_csv(file_dir)[[\"tweet\", \"mentions\", \"hashtags\"]]\n",
    "tweets = data['tweet']\n",
    "tweets.iloc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweet text\n",
    "\n",
    "Here, we apply the following measures:\n",
    "- Stop-word removal\n",
    "- Stemming\n",
    "- Spelling Mistakes (TODO)\n",
    "- Slang             (TODO)\n",
    "\n",
    "Some other info on Tweet Cleaning:\n",
    "\n",
    "https://www.kaggle.com/ragnisah/text-data-cleaning-tweets-analysis\n",
    "https://kite.com/python/docs/nltk.TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Wow, thank you — on my way!! https://twitter.com/teamtrump/status/1230581311464787968 …\n"
    },
    {
     "data": {
      "text/plain": "['wow', 'thank', 'way']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# TODO  Keep some punctuation\n",
    "#       Re-Define Stopwords\n",
    "def clean_text(text):                                                               # Line-by-line, this code:\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)    #   - Removes URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)                                             #   - Removes Punctuation\n",
    "    text = text.replace(u'\\xa0', u' ')                                              #   - Removes Break lines\n",
    "    text = text.split(' ')                                                          #   - Splits up text by Whitespace\n",
    "    text = [word for word in text if word not in stop_words]                        #   - Removes Stop-Words\n",
    "    text = [ps.stem(word) for word in text]                                         #   - Stems words\n",
    "    return list(filter(lambda a: a != '', text))                                    #   - Removes white space\n",
    "\n",
    "test_tweet = tweets[30]\n",
    "\n",
    "print(test_tweet)\n",
    "clean_text(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'be'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('being')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37464bitf11fe8824d0244f597f4f929b637ffe8",
   "language": "python",
   "display_name": "Python 3.7.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}